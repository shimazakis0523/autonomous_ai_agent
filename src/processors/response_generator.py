"""
è‡ªå¾‹å‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - å¿œç­”ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³
ãƒ•ã‚§ãƒ¼ã‚º7: ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘æœ€çµ‚å¿œç­”ã®ç”Ÿæˆ
ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°æ©Ÿèƒ½ä»˜ã
"""
import json
import logging
import re
from typing import Dict, List, Any, Optional
from datetime import datetime
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_openai import ChatOpenAI

from ..core.agent_state import AgentState, AgentPhase, add_error_context

logger = logging.getLogger(__name__)


class ResponseGenerator:
    """å¿œç­”ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, llm: ChatOpenAI, trace_logger=None):
        """
        å¿œç­”ç”Ÿæˆã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
        
        Args:
            llm: è¨€èªãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            trace_logger: ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ãƒ­ã‚¬ãƒ¼ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        """
        self.llm = llm
        self.trace_logger = trace_logger
        self.max_response_length = 2000
        self.min_response_length = 50
        
        # å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        self.response_styles = {
            "professional": {
                "tone": "ä¸å¯§ã§å°‚é–€çš„",
                "format": "æ§‹é€ åŒ–ã•ã‚ŒãŸèª¬æ˜",
                "emoji_usage": "æ§ãˆã‚"
            },
            "friendly": {
                "tone": "è¦ªã—ã¿ã‚„ã™ãåˆ†ã‹ã‚Šã‚„ã™ã„",
                "format": "ä¼šè©±çš„ãªèª¬æ˜",
                "emoji_usage": "é©åº¦ã«ä½¿ç”¨"
            },
            "technical": {
                "tone": "æŠ€è¡“çš„ã§è©³ç´°",
                "format": "ã‚³ãƒ¼ãƒ‰ã‚„ä¾‹ã‚’å«ã‚€",
                "emoji_usage": "æœ€å°é™"
            },
            "educational": {
                "tone": "æ•™è‚²çš„ã§æ®µéšçš„",
                "format": "ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—",
                "emoji_usage": "ç†è§£ã‚’åŠ©ã‘ã‚‹ç¨‹åº¦"
            }
        }
        
        # å“è³ªãƒã‚§ãƒƒã‚¯é …ç›®
        self.quality_checks = [
            "completeness_check",    # å®Œå…¨æ€§
            "accuracy_check",        # æ­£ç¢ºæ€§
            "relevance_check",       # é–¢é€£æ€§
            "clarity_check",         # æ˜ç¢ºæ€§
            "tone_consistency_check" # ãƒˆãƒ¼ãƒ³ä¸€è²«æ€§
        ]
        
        if self.trace_logger:
            self.trace_logger.log_custom_event(
                "RESPONSE_GENERATOR_INIT",
                "ResponseGeneratoråˆæœŸåŒ–å®Œäº†",
                {
                    "response_styles": list(self.response_styles.keys()),
                    "quality_checks": self.quality_checks,
                    "llm_model": getattr(llm, 'model_name', 'unknown')
                }
            )
        
    async def generate_response(self, state: AgentState) -> AgentState:
        """
        æœ€çµ‚ãƒ¦ãƒ¼ã‚¶ãƒ¼å¿œç­”ã®ç”Ÿæˆ
        
        Args:
            state: ç¾åœ¨ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçŠ¶æ…‹
            
        Returns:
            æœ€çµ‚å¿œç­”ã‚’å«ã‚€æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """
        try:
            processed_result = state.get("processed_result", {})
            
            if not processed_result:
                logger.warning("å‡¦ç†æ¸ˆã¿çµæœãŒã‚ã‚Šã¾ã›ã‚“")
                return self._create_fallback_response(state)
            
            logger.info("æœ€çµ‚å¿œç­”ã‚’ç”Ÿæˆä¸­...")
            
            # 1. å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã®æ±ºå®š
            response_style = self._determine_response_style(state)
            
            # 2. ä¸»è¦å¿œç­”ã®ç”Ÿæˆ
            main_response = await self._generate_main_response(state, response_style)
            
            # 3. è£œè¶³æƒ…å ±ã®ç”Ÿæˆ
            supplementary_info = self._generate_supplementary_info(state)
            
            # 4. å¿œç­”ã®æœ€é©åŒ–
            optimized_response = await self._optimize_response(main_response, state)
            
            # 5. æœ€çµ‚å¿œç­”ã®æ§‹ç¯‰
            final_response = self._build_final_response(
                optimized_response, supplementary_info, state
            )
            
            # 6. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å‡ºåŠ›
            self._display_response_to_user(final_response)
            
            logger.info("å¿œç­”ç”Ÿæˆå®Œäº†")
            
            # çŠ¶æ…‹ã®æ›´æ–°
            updated_state = state.copy()
            updated_state.update({
                "final_response": final_response,
                "current_phase": AgentPhase.COMPLETED.value,
                "messages": state["messages"] + [
                    AIMessage(content=final_response["main_content"])
                ]
            })
            
            return updated_state
            
        except Exception as e:
            logger.error(f"å¿œç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {str(e)}")
            return add_error_context(state, "response_generation", str(e))
    
    def _determine_response_style(self, state: AgentState) -> Dict[str, Any]:
        """å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«ã®æ±ºå®š"""
        intent_analysis = state.get("intent_analysis", {})
        confidence_assessment = state.get("processed_result", {}).get("confidence_assessment", {})
        
        # åŸºæœ¬ã‚¹ã‚¿ã‚¤ãƒ«è¨­å®š
        style = {
            "tone": "professional",  # professional, casual, technical
            "detail_level": "medium",  # brief, medium, detailed
            "format": "structured",  # narrative, structured, bullet_points
            "include_confidence": True,
            "include_next_steps": True
        }
        
        # æ„å›³ã«åŸºã¥ãã‚¹ã‚¿ã‚¤ãƒ«èª¿æ•´
        primary_intent = intent_analysis.get("primary_intent", "conversation")
        user_expertise = intent_analysis.get("user_expertise", "intermediate")
        
        if primary_intent == "code_generation":
            style["format"] = "code_focused"
            style["detail_level"] = "detailed"
            
        elif primary_intent == "conversation":
            style["tone"] = "casual"
            style["format"] = "narrative"
            
        elif primary_intent in ["data_analysis", "research_analysis"]:
            style["tone"] = "technical"
            style["detail_level"] = "detailed"
            style["format"] = "structured"
        
        # å°‚é–€ãƒ¬ãƒ™ãƒ«ã«åŸºã¥ãèª¿æ•´
        if user_expertise == "beginner":
            style["detail_level"] = "detailed"
            style["tone"] = "educational"
        elif user_expertise == "expert":
            style["detail_level"] = "brief"
            style["tone"] = "technical"
        
        # ä¿¡é ¼åº¦ã«åŸºã¥ãèª¿æ•´
        confidence = confidence_assessment.get("overall_confidence", 0.5)
        if confidence < 0.5:
            style["include_confidence"] = True
            style["include_disclaimers"] = True
        
        return style
    
    async def _generate_main_response(self, state: AgentState, 
                                    response_style: Dict[str, Any]) -> str:
        """ä¸»è¦å¿œç­”ã®ç”Ÿæˆ"""
        user_input = state["user_input"]
        processed_result = state["processed_result"]
        intent_analysis = state.get("intent_analysis", {})
        
        # çµ±åˆã•ã‚ŒãŸç™ºè¦‹ã¨æ´å¯Ÿã®æŠ½å‡º
        integrated_findings = processed_result.get("integrated_findings", {})
        insights = processed_result.get("insights_and_analysis", {})
        confidence_assessment = processed_result.get("confidence_assessment", {})
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰
        response_prompt = self._build_response_prompt(
            user_input, integrated_findings, insights, confidence_assessment, 
            intent_analysis, response_style
        )
        
        try:
            messages = [
                SystemMessage(content=self._get_system_prompt(response_style)),
                HumanMessage(content=response_prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            main_response = response.content
            
            # å¿œç­”é•·ã®ç¢ºèª
            if len(main_response) < self.min_response_length:
                logger.warning("å¿œç­”ãŒçŸ­ã™ãã¾ã™ã€‚è©³ç´°åŒ–ã‚’è©¦è¡Œ")
                main_response = await self._expand_response(main_response, state)
            elif len(main_response) > self.max_response_length:
                logger.warning("å¿œç­”ãŒé•·ã™ãã¾ã™ã€‚è¦ç´„ã‚’è©¦è¡Œ")
                main_response = await self._summarize_response(main_response, state)
            
            return main_response
            
        except Exception as e:
            logger.warning(f"ä¸»è¦å¿œç­”ç”Ÿæˆã«å¤±æ•—: {str(e)}")
            return self._create_basic_response(integrated_findings, user_input)
    
    def _build_response_prompt(self, user_input: str, integrated_findings: Dict[str, Any],
                             insights: Dict[str, Any], confidence_assessment: Dict[str, Any],
                             intent_analysis: Dict[str, Any], response_style: Dict[str, Any]) -> str:
        """å¿œç­”ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ§‹ç¯‰"""
        
        main_findings = integrated_findings.get("main_findings", [])
        key_insights = insights.get("key_insights", [])
        confidence = confidence_assessment.get("overall_confidence", 0.5)
        reliability_level = confidence_assessment.get("reliability_level", "ä¸­ä¿¡é ¼åº¦")
        
        prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«å¯¾ã—ã¦ã€ä»¥ä¸‹ã®åˆ†æçµæœã«åŸºã¥ã„ã¦åŒ…æ‹¬çš„ã§æœ‰ç”¨ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼š

**ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•:**
{user_input}

**ä¸»è¦ãªç™ºè¦‹:**
{chr(10).join(f"â€¢ {finding}" for finding in main_findings)}

**é‡è¦ãªæ´å¯Ÿ:**
{chr(10).join(f"â€¢ {insight}" for insight in key_insights)}

**ä¿¡é ¼åº¦æƒ…å ±:**
- å…¨ä½“ä¿¡é ¼åº¦: {confidence:.1%}
- ä¿¡é ¼æ€§ãƒ¬ãƒ™ãƒ«: {reliability_level}

**å¿œç­”ã‚¹ã‚¿ã‚¤ãƒ«æŒ‡ç¤º:**
- ãƒˆãƒ¼ãƒ³: {response_style.get('tone', 'professional')}
- è©³ç´°ãƒ¬ãƒ™ãƒ«: {response_style.get('detail_level', 'medium')}
- ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ: {response_style.get('format', 'structured')}

**å¿œç­”è¦ä»¶:**
1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«ç›´æ¥ç­”ãˆã‚‹
2. ä¸»è¦ãªç™ºè¦‹ã‚’åˆ†ã‹ã‚Šã‚„ã™ãèª¬æ˜ã™ã‚‹
3. å®Ÿç”¨çš„ãªæ´å¯Ÿã‚„æ¨å¥¨äº‹é …ã‚’æä¾›ã™ã‚‹
4. å¿…è¦ã«å¿œã˜ã¦åˆ¶é™äº‹é …ã‚„æ³¨æ„ç‚¹ã‚’æ˜è¨˜ã™ã‚‹
5. è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„æ–‡ç« ã«ã™ã‚‹

å°‚é–€çš„ã§æ­£ç¢ºã€ã‹ã¤ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã¨ã£ã¦ä¾¡å€¤ã®ã‚ã‚‹å›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
        """
        
        return prompt.strip()
    
    def _get_system_prompt(self, response_style: Dict[str, Any]) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å–å¾—"""
        base_prompt = "ã‚ãªãŸã¯é«˜åº¦ãªAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
        
        tone = response_style.get("tone", "professional")
        if tone == "technical":
            base_prompt += "æŠ€è¡“çš„ã«æ­£ç¢ºã§è©³ç´°ãªæƒ…å ±ã‚’æä¾›ã—ã€å°‚é–€ç”¨èªã‚’é©åˆ‡ã«ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚"
        elif tone == "casual":
            base_prompt += "è¦ªã—ã¿ã‚„ã™ãã€åˆ†ã‹ã‚Šã‚„ã™ã„è¨€è‘‰ã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        elif tone == "educational":
            base_prompt += "æ•™è‚²çš„ã§æ®µéšçš„ãªèª¬æ˜ã‚’å¿ƒãŒã‘ã€åˆå¿ƒè€…ã«ã‚‚ç†è§£ã—ã‚„ã™ãã—ã¦ãã ã•ã„ã€‚"
        else:
            base_prompt += "å°‚é–€çš„ã§ä¿¡é ¼æ€§ãŒé«˜ãã€æ˜ç¢ºã§ç°¡æ½”ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚"
        
        base_prompt += "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®çœŸã®ãƒ‹ãƒ¼ã‚ºã‚’ç†è§£ã—ã€å®Ÿè¡Œå¯èƒ½ã§ä¾¡å€¤ã®ã‚ã‚‹æƒ…å ±ã‚’æä¾›ã™ã‚‹ã“ã¨ã‚’æœ€å„ªå…ˆã«ã—ã¦ãã ã•ã„ã€‚"
        
        return base_prompt
    
    async def _optimize_response(self, main_response: str, state: AgentState) -> str:
        """å¿œç­”ã®æœ€é©åŒ–"""
        # åŸºæœ¬çš„ãªæœ€é©åŒ–ãƒ«ãƒ¼ãƒ«
        
        # 1. å†—é•·æ€§ã®é™¤å»
        optimized = self._remove_redundancy(main_response)
        
        # 2. æ§‹é€ ã®æ”¹å–„
        optimized = self._improve_structure(optimized)
        
        # 3. èª­ã¿ã‚„ã™ã•ã®å‘ä¸Š
        optimized = self._improve_readability(optimized)
        
        return optimized
    
    def _generate_supplementary_info(self, state: AgentState) -> Dict[str, Any]:
        """è£œè¶³æƒ…å ±ã®ç”Ÿæˆ"""
        processed_result = state.get("processed_result", {})
        confidence_assessment = processed_result.get("confidence_assessment", {})
        execution_summary = state.get("task_results", {}).get("execution_summary", {})
        
        supplementary = {}
        
        # ä¿¡é ¼åº¦æƒ…å ±
        if confidence_assessment:
            confidence = confidence_assessment.get("overall_confidence", 0)
            reliability = confidence_assessment.get("reliability_level", "ä¸æ˜")
            supplementary["confidence_info"] = {
                "level": f"{confidence:.1%}",
                "reliability": reliability,
                "recommendation": confidence_assessment.get("recommendation", "")
            }
        
        # å®Ÿè¡Œçµ±è¨ˆ
        if execution_summary:
            supplementary["execution_stats"] = {
                "total_tasks": execution_summary.get("total_tasks", 0),
                "success_rate": f"{execution_summary.get('success_rate', 0):.1%}",
                "execution_time": f"{execution_summary.get('execution_time', 0):.1f}ç§’"
            }
        
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ææ¡ˆ
        intent_analysis = state.get("intent_analysis", {})
        if intent_analysis.get("complexity") in ["high", "critical"]:
            supplementary["next_steps"] = [
                "çµæœã®è©³ç´°ç¢ºèª",
                "è¿½åŠ ã®åˆ†ææ¤œè¨",
                "å°‚é–€å®¶ã¸ã®ç›¸è«‡"
            ]
        else:
            supplementary["next_steps"] = [
                "è¿½åŠ è³ªå•ãŒã‚ã‚Œã°é æ…®ãªãã©ã†ã",
                "ä»–ã®é–¢é€£ãƒˆãƒ”ãƒƒã‚¯ã«ã¤ã„ã¦ã‚‚è³ªå•å¯èƒ½"
            ]
        
        return supplementary
    
    def _build_final_response(self, main_response: str, supplementary_info: Dict[str, Any],
                            state: AgentState) -> Dict[str, Any]:
        """æœ€çµ‚å¿œç­”ã®æ§‹ç¯‰"""
        confidence_info = supplementary_info.get("confidence_info", {})
        
        return {
            "main_content": main_response,
            "confidence_level": confidence_info.get("level", "ä¸æ˜"),
            "reliability": confidence_info.get("reliability", "ä¸æ˜"),
            "execution_stats": supplementary_info.get("execution_stats", {}),
            "next_steps": supplementary_info.get("next_steps", []),
            "generated_at": datetime.now().isoformat(),
            "response_type": "comprehensive_analysis",
            "metadata": {
                "user_input": state["user_input"],
                "session_id": state["session_metadata"]["session_id"],
                "processing_time": self._calculate_total_processing_time(state)
            }
        }
    
    def _display_response_to_user(self, final_response: Dict[str, Any]) -> None:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å¿œç­”è¡¨ç¤º"""
        print("\n" + "="*80)
        print("ğŸ¤– è‡ªå¾‹å‹AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ - å›ç­”")
        print("="*80)
        
        # ãƒ¡ã‚¤ãƒ³å¿œç­”
        print(final_response["main_content"])
        
        # ä¿¡é ¼åº¦æƒ…å ±
        if final_response.get("confidence_level"):
            print(f"\nğŸ“Š ä¿¡é ¼åº¦: {final_response['confidence_level']} ({final_response.get('reliability', 'N/A')})")
        
        # å®Ÿè¡Œçµ±è¨ˆ
        exec_stats = final_response.get("execution_stats", {})
        if exec_stats:
            print(f"\nâš¡ å®Ÿè¡Œçµ±è¨ˆ:")
            print(f"   â€¢ å‡¦ç†ã‚¿ã‚¹ã‚¯æ•°: {exec_stats.get('total_tasks', 'N/A')}")
            print(f"   â€¢ æˆåŠŸç‡: {exec_stats.get('success_rate', 'N/A')}")
            print(f"   â€¢ å®Ÿè¡Œæ™‚é–“: {exec_stats.get('execution_time', 'N/A')}")
        
        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—
        next_steps = final_response.get("next_steps", [])
        if next_steps:
            print(f"\nğŸ’¡ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            for step in next_steps:
                print(f"   â€¢ {step}")
        
        print("\n" + "="*80)
    
    def _remove_redundancy(self, text: str) -> str:
        """å†—é•·æ€§ã®é™¤å»"""
        # åŒã˜æ–‡ã®ç¹°ã‚Šè¿”ã—ã‚’é™¤å»ï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
        sentences = text.split('ã€‚')
        unique_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and sentence not in unique_sentences:
                unique_sentences.append(sentence)
        
        return 'ã€‚'.join(unique_sentences)
    
    def _improve_structure(self, text: str) -> str:
        """æ§‹é€ ã®æ”¹å–„"""
        # æ®µè½ã®æ•´ç†ï¼ˆç°¡å˜ãªå®Ÿè£…ï¼‰
        lines = text.split('\n')
        improved_lines = []
        
        for line in lines:
            line = line.strip()
            if line:
                improved_lines.append(line)
        
        return '\n\n'.join(improved_lines)
    
    def _improve_readability(self, text: str) -> str:
        """èª­ã¿ã‚„ã™ã•ã®å‘ä¸Š"""
        # åŸºæœ¬çš„ãªèª­ã¿ã‚„ã™ã•ã®æ”¹å–„
        improved = text.replace('ã€‚\n', 'ã€‚\n\n')  # æ®µè½é–“ã®é–“éš”
        improved = improved.replace('ï¼š', ':\n')  # ãƒªã‚¹ãƒˆå½¢å¼ã®æ”¹å–„
        
        return improved
    
    async def _expand_response(self, short_response: str, state: AgentState) -> str:
        """çŸ­ã„å¿œç­”ã®è©³ç´°åŒ–"""
        expand_prompt = f"""
ä»¥ä¸‹ã®å›ç­”ã‚’ã‚ˆã‚Šè©³ç´°ã§æœ‰ç”¨ãªã‚‚ã®ã«æ‹¡å¼µã—ã¦ãã ã•ã„ï¼š

{short_response}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…ƒã®è³ªå•: {state['user_input']}

è©³ç´°ãªèª¬æ˜ã€å…·ä½“ä¾‹ã€å®Ÿç”¨çš„ãªã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’è¿½åŠ ã—ã¦ã€{self.min_response_length}æ–‡å­—ä»¥ä¸Šã®åŒ…æ‹¬çš„ãªå›ç­”ã«ã—ã¦ãã ã•ã„ã€‚
        """
        
        try:
            response = await self.llm.ainvoke([
                SystemMessage(content="ç°¡æ½”ãªå›ç­”ã‚’è©³ç´°ã§æœ‰ç”¨ãªå›ç­”ã«æ‹¡å¼µã—ã¦ãã ã•ã„ã€‚"),
                HumanMessage(content=expand_prompt)
            ])
            return response.content
        except Exception:
            return short_response  # æ‹¡å¼µã«å¤±æ•—ã—ãŸå ´åˆã¯å…ƒã®å¿œç­”ã‚’è¿”ã™
    
    async def _summarize_response(self, long_response: str, state: AgentState) -> str:
        """é•·ã„å¿œç­”ã®è¦ç´„"""
        summarize_prompt = f"""
ä»¥ä¸‹ã®å›ç­”ã‚’{self.max_response_length}æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªæƒ…å ±ã¯ä¿æŒã—ã€ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ãã—ã¦ãã ã•ã„ï¼š

{long_response}
        """
        
        try:
            response = await self.llm.ainvoke([
                SystemMessage(content="é•·ã„å›ç­”ã‚’ç°¡æ½”ã§åˆ†ã‹ã‚Šã‚„ã™ãè¦ç´„ã—ã¦ãã ã•ã„ã€‚"),
                HumanMessage(content=summarize_prompt)
            ])
            return response.content
        except Exception:
            # è¦ç´„ã«å¤±æ•—ã—ãŸå ´åˆã¯å…ˆé ­ã‹ã‚‰åˆ‡ã‚Šå–ã‚Š
            return long_response[:self.max_response_length] + "..."
    
    def _create_basic_response(self, integrated_findings: Dict[str, Any], user_input: str) -> str:
        """åŸºæœ¬çš„ãªå¿œç­”ã®ä½œæˆ"""
        main_findings = integrated_findings.get("main_findings", [])
        summary = integrated_findings.get("summary", "")
        
        if main_findings:
            response = f"ã”è³ªå•ã€Œ{user_input}ã€ã«ã¤ã„ã¦ã€ä»¥ä¸‹ã®çµæœã‚’ãŠä¼ãˆã—ã¾ã™ï¼š\n\n"
            for i, finding in enumerate(main_findings[:3], 1):
                response += f"{i}. {finding}\n"
            
            if summary:
                response += f"\n{summary}"
            
            return response
        else:
            return f"ã”è³ªå•ã€Œ{user_input}ã€ã«ã¤ã„ã¦å‡¦ç†ã‚’è¡Œã„ã¾ã—ãŸãŒã€æ˜ç¢ºãªçµæœãŒå¾—ã‚‰ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚’ã—ã¦ã„ãŸã ã‘ã‚‹ã¨ã€ã‚ˆã‚Šè‰¯ã„å›ç­”ã‚’æä¾›ã§ãã¾ã™ã€‚"
    
    def _create_fallback_response(self, state: AgentState) -> AgentState:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¿œç­”ã®ä½œæˆ"""
        user_input = state["user_input"]
        
        fallback_response = {
            "main_content": f"ã”è³ªå•ã€Œ{user_input}ã€ã«ã¤ã„ã¦å‡¦ç†ã‚’è©¦ã¿ã¾ã—ãŸãŒã€ååˆ†ãªçµæœã‚’å¾—ã‚‹ã“ã¨ãŒã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ç”³ã—è¨³ã”ã–ã„ã¾ã›ã‚“ã€‚ã‚ˆã‚Šå…·ä½“çš„ãªè³ªå•ã‚„ã€åˆ¥ã®æ–¹æ³•ã§ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ãŠè©¦ã—ãã ã•ã„ã€‚",
            "confidence_level": "ä½",
            "reliability": "è¦æ³¨æ„",
            "generated_at": datetime.now().isoformat(),
            "response_type": "fallback"
        }
        
        self._display_response_to_user(fallback_response)
        
        updated_state = state.copy()
        updated_state.update({
            "final_response": fallback_response,
            "current_phase": AgentPhase.COMPLETED.value
        })
        
        return updated_state
    
    def _calculate_total_processing_time(self, state: AgentState) -> float:
        """ç·å‡¦ç†æ™‚é–“ã®è¨ˆç®—"""
        session_start = state["session_metadata"].get("created_at")
        if session_start:
            start_time = datetime.fromisoformat(session_start)
            current_time = datetime.now()
            total_time = (current_time - start_time).total_seconds()
            return total_time
        return 0.0 